{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: LightGCN baseline for 200-20 reranking\n",
    "\n",
    "Goal: train LightGCN on user-edition interactions, score the provided 200 candidates per user, and apply a simple genre-diversity rerank to produce `submission.csv`.\n",
    "\n",
    "Notes:\n",
    "- Only two event types exist in the data: `wishlist` (1) and `read` (2).\n",
    "- The competition metric is `0.7 * NDCG@20 + 0.3 * Diversity@20` where Diversity uses genres.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup: imports and reproducibility\n",
    "# If needed:\n",
    "# pip install pandas numpy scipy torch tqdm\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy import sparse\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Expected repo structure:\n",
    "- `data/` with `users.csv`, `interactions.csv`, `editions.csv`, `book_genres.csv`\n",
    "- `submit/` with `targets.csv`, `candidates.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "SUBMIT_DIR = \"submit\"\n",
    "\n",
    "users = pd.read_csv(os.path.join(DATA_DIR, \"users.csv\"))\n",
    "interactions = pd.read_csv(os.path.join(DATA_DIR, \"interactions.csv\"), parse_dates=[\"event_ts\"])\n",
    "editions = pd.read_csv(os.path.join(DATA_DIR, \"editions.csv\"))\n",
    "book_genres = pd.read_csv(os.path.join(DATA_DIR, \"book_genres.csv\"))\n",
    "\n",
    "targets = pd.read_csv(os.path.join(SUBMIT_DIR, \"targets.csv\"))\n",
    "candidates = pd.read_csv(os.path.join(SUBMIT_DIR, \"candidates.csv\"))\n",
    "\n",
    "print(\"users\", users.shape)\n",
    "print(\"interactions\", interactions.shape)\n",
    "print(\"editions\", editions.shape)\n",
    "print(\"book_genres\", book_genres.shape)\n",
    "print(\"targets\", targets.shape)\n",
    "print(\"candidates\", candidates.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID mappings\n",
    "\n",
    "We map all user_id and edition_id to contiguous indices for model embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_user_ids = pd.Index(pd.concat([users[\"user_id\"], interactions[\"user_id\"], targets[\"user_id\"]]).unique())\n",
    "all_edition_ids = pd.Index(pd.concat([editions[\"edition_id\"], interactions[\"edition_id\"], candidates[\"edition_id\"]]).unique())\n",
    "\n",
    "user2idx = {u: i for i, u in enumerate(all_user_ids)}\n",
    "item2idx = {it: i for i, it in enumerate(all_edition_ids)}\n",
    "idx2item = {i: it for it, i in item2idx.items()}\n",
    "\n",
    "n_users = len(all_user_ids)\n",
    "n_items = len(all_edition_ids)\n",
    "\n",
    "interactions[\"u\"] = interactions[\"user_id\"].map(user2idx).astype(np.int64)\n",
    "interactions[\"i\"] = interactions[\"edition_id\"].map(item2idx).astype(np.int64)\n",
    "\n",
    "print(\"n_users\", n_users, \"n_items\", n_items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction weights\n",
    "\n",
    "We use a simple weighting scheme:\n",
    "- wishlist -> 1\n",
    "- read -> 3 (+ small rating boost)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_weight(df: pd.DataFrame) -> np.ndarray:\n",
    "    w = np.zeros(len(df), dtype=np.float32)\n",
    "    is_wish = df[\"event_type\"] == 1\n",
    "    is_read = df[\"event_type\"] == 2\n",
    "\n",
    "    w[is_wish] = 1.0\n",
    "    w[is_read] = 3.0\n",
    "\n",
    "    # rating only for read; normalize 0..5 to 0..1\n",
    "    r = df[\"rating\"].astype(\"float32\").fillna(0.0).clip(0.0, 5.0) / 5.0\n",
    "    w[is_read] += 0.2 * r[is_read].to_numpy()\n",
    "    return w\n",
    "\n",
    "interactions[\"w\"] = make_weight(interactions)\n",
    "interactions[[\"user_id\", \"edition_id\", \"event_type\", \"rating\", \"w\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional local time split (30 days)\n",
    "\n",
    "For offline validation you can split per user by the last 30 days.\n",
    "Set `DO_LOCAL_SPLIT = False` to train on all data.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "DO_LOCAL_SPLIT = True\n",
    "\n",
    "if DO_LOCAL_SPLIT:\n",
    "    interactions = interactions.sort_values([\"u\", \"event_ts\"])\n",
    "    max_ts = interactions.groupby(\"u\")[\"event_ts\"].transform(\"max\")\n",
    "    cutoff = max_ts - pd.Timedelta(days=30)\n",
    "\n",
    "    train_df = interactions[interactions[\"event_ts\"] < cutoff].copy()\n",
    "    val_df = interactions[interactions[\"event_ts\"] >= cutoff].copy()\n",
    "else:\n",
    "    train_df = interactions.copy()\n",
    "    val_df = None\n",
    "\n",
    "print(\"train events\", len(train_df), \"val events\", 0 if val_df is None else len(val_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build normalized adjacency for LightGCN\n",
    "\n",
    "We build the bipartite graph adjacency and use D^{-1/2} A D^{-1/2} normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_norm_adj(n_users, n_items, df):\n",
    "    ui = df[[\"u\", \"i\"]].to_numpy()\n",
    "    w = df[\"w\"].to_numpy()\n",
    "\n",
    "    R = sparse.coo_matrix((w, (ui[:, 0], ui[:, 1])), shape=(n_users, n_items))\n",
    "\n",
    "    upper = sparse.hstack([sparse.csr_matrix((n_users, n_users)), R], format=\"csr\")\n",
    "    lower = sparse.hstack([R.T, sparse.csr_matrix((n_items, n_items))], format=\"csr\")\n",
    "    A = sparse.vstack([upper, lower], format=\"csr\")\n",
    "\n",
    "    deg = np.array(A.sum(axis=1)).squeeze()\n",
    "    deg_inv_sqrt = np.power(deg, -0.5, where=(deg > 0))\n",
    "    deg_inv_sqrt[~np.isfinite(deg_inv_sqrt)] = 0.0\n",
    "    D_inv_sqrt = sparse.diags(deg_inv_sqrt)\n",
    "\n",
    "    A_norm = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return A_norm.tocoo()\n",
    "\n",
    "A_norm = build_norm_adj(n_users, n_items, train_df)\n",
    "print(\"A_norm nnz\", A_norm.nnz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGCN model and BPR training\n",
    "\n",
    "This is a simple LightGCN implementation with BPR loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scipy_coo_to_torch_sparse(coo: sparse.coo_matrix, device):\n",
    "    idx = np.vstack([coo.row, coo.col]).astype(np.int64)\n",
    "    val = coo.data.astype(np.float32)\n",
    "    i = torch.from_numpy(idx).to(device)\n",
    "    v = torch.from_numpy(val).to(device)\n",
    "    return torch.sparse_coo_tensor(i, v, size=coo.shape, device=device).coalesce()\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim=64, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "        nn.init.normal_(self.user_emb.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_emb.weight, std=0.01)\n",
    "\n",
    "    def propagate(self, A_norm_sp):\n",
    "        x0 = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
    "        xs = [x0]\n",
    "        x = x0\n",
    "        for _ in range(self.n_layers):\n",
    "            x = torch.sparse.mm(A_norm_sp, x)\n",
    "            xs.append(x)\n",
    "        x_out = torch.mean(torch.stack(xs, dim=0), dim=0)\n",
    "        users_out = x_out[:self.n_users]\n",
    "        items_out = x_out[self.n_users:]\n",
    "        return users_out, items_out\n",
    "\n",
    "def bpr_loss(u_vec, pos_vec, neg_vec):\n",
    "    pos = torch.sum(u_vec * pos_vec, dim=1)\n",
    "    neg = torch.sum(u_vec * neg_vec, dim=1)\n",
    "    return -torch.mean(F.logsigmoid(pos - neg))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "A_torch = scipy_coo_to_torch_sparse(A_norm, device)\n",
    "\n",
    "model = LightGCN(n_users, n_items, emb_dim=96, n_layers=3).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=3e-3, weight_decay=1e-6)\n",
    "\n",
    "user_pos = train_df.groupby(\"u\")[\"i\"].apply(lambda s: set(s.values)).to_dict()\n",
    "\n",
    "all_items = np.arange(n_items)\n",
    "\n",
    "def sample_batch(batch_size=4096):\n",
    "    us = np.random.choice(list(user_pos.keys()), size=batch_size, replace=True)\n",
    "    pos = np.empty(batch_size, dtype=np.int64)\n",
    "    neg = np.empty(batch_size, dtype=np.int64)\n",
    "\n",
    "    for t, u in enumerate(us):\n",
    "        p = np.random.choice(list(user_pos[u]))\n",
    "        pos[t] = p\n",
    "        while True:\n",
    "            j = np.random.randint(0, n_items)\n",
    "            if j not in user_pos[u]:\n",
    "                neg[t] = j\n",
    "                break\n",
    "    return us, pos, neg\n",
    "\n",
    "EPOCHS = 5\n",
    "STEPS_PER_EPOCH = 300\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "\n",
    "    for _ in tqdm(range(STEPS_PER_EPOCH), desc=f\"epoch {epoch}\"):\n",
    "        us, pos, neg = sample_batch()\n",
    "        us = torch.from_numpy(us).to(device)\n",
    "        pos = torch.from_numpy(pos).to(device)\n",
    "        neg = torch.from_numpy(neg).to(device)\n",
    "\n",
    "        users_out, items_out = model.propagate(A_torch)\n",
    "        u_vec = users_out[us]\n",
    "        p_vec = items_out[pos]\n",
    "        n_vec = items_out[neg]\n",
    "\n",
    "        loss = bpr_loss(u_vec, p_vec, n_vec)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total += float(loss.item())\n",
    "\n",
    "    print(f\"epoch {epoch} loss={total / STEPS_PER_EPOCH:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the 200 candidates\n",
    "\n",
    "We compute dot-product scores for each (user, edition) candidate pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def score_candidates(model, A_torch, candidates_df: pd.DataFrame):\n",
    "    model.eval()\n",
    "    users_out, items_out = model.propagate(A_torch)\n",
    "\n",
    "    cand = candidates_df.copy()\n",
    "    cand[\"u\"] = cand[\"user_id\"].map(user2idx).astype(np.int64)\n",
    "    cand[\"i\"] = cand[\"edition_id\"].map(item2idx).astype(np.int64)\n",
    "\n",
    "    u_idx = torch.from_numpy(cand[\"u\"].values).to(device)\n",
    "    i_idx = torch.from_numpy(cand[\"i\"].values).to(device)\n",
    "\n",
    "    scores = torch.sum(users_out[u_idx] * items_out[i_idx], dim=1).detach().cpu().numpy()\n",
    "    cand[\"score\"] = scores\n",
    "    return cand[[\"user_id\", \"edition_id\", \"score\"]]\n",
    "\n",
    "cand_scored = score_candidates(model, A_torch, candidates)\n",
    "cand_scored.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre-diversity rerank (greedy)\n",
    "\n",
    "We rerank the scored candidates with a simple greedy MMR-like objective that adds genre coverage and intra-list diversity on top of the relevance score.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "ed2book = dict(zip(editions[\"edition_id\"].values, editions[\"book_id\"].values))\n",
    "bg = book_genres.groupby(\"book_id\")[\"genre_id\"].apply(lambda s: set(s.values)).to_dict()\n",
    "\n",
    "ed2genres = {}\n",
    "for ed in all_edition_ids:\n",
    "    b = ed2book.get(ed, None)\n",
    "    ed2genres[ed] = bg.get(b, set())\n",
    "\n",
    "def jaccard_dist(a: set, b: set) -> float:\n",
    "    if not a and not b:\n",
    "        return 0.0\n",
    "    inter = len(a & b)\n",
    "    union = len(a | b)\n",
    "    return 1.0 - (inter / union if union else 0.0)\n",
    "\n",
    "def rerank_diverse(df_user: pd.DataFrame, topk=20, lam=0.15, gamma=0.5):\n",
    "    items = df_user.sort_values(\"score\", ascending=False)[[\"edition_id\", \"score\"]].to_records(index=False)\n",
    "\n",
    "    chosen = []\n",
    "    chosen_genres = set()\n",
    "\n",
    "    for _ in range(topk):\n",
    "        best = None\n",
    "        best_val = -1e18\n",
    "\n",
    "        for ed, s in items:\n",
    "            if ed in chosen:\n",
    "                continue\n",
    "\n",
    "            g = ed2genres.get(ed, set())\n",
    "            if len(g) > 0:\n",
    "                new = len(g - chosen_genres) / len(g)\n",
    "            else:\n",
    "                new = 0.0\n",
    "\n",
    "            if not chosen:\n",
    "                ild = 0.0\n",
    "            else:\n",
    "                dsum = 0.0\n",
    "                for prev in chosen:\n",
    "                    dsum += jaccard_dist(g, ed2genres.get(prev, set()))\n",
    "                ild = dsum / len(chosen)\n",
    "\n",
    "            val = float(s) + lam * (new + gamma * ild)\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best = ed\n",
    "\n",
    "        chosen.append(best)\n",
    "        chosen_genres |= ed2genres.get(best, set())\n",
    "\n",
    "    return chosen\n",
    "\n",
    "pred_rows = []\n",
    "for u, grp in tqdm(cand_scored.groupby(\"user_id\"), total=cand_scored[\"user_id\"].nunique()):\n",
    "    chosen = rerank_diverse(grp, topk=20, lam=0.15, gamma=0.5)\n",
    "    for r, ed in enumerate(chosen, start=1):\n",
    "        pred_rows.append((u, ed, r))\n",
    "\n",
    "submission = pd.DataFrame(pred_rows, columns=[\"user_id\", \"edition_id\", \"rank\"])\n",
    "print(submission.shape)\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save submission\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "out_path = \"submission.csv\"\n",
    "submission.to_csv(out_path, index=False)\n",
    "print(\"saved\", out_path)\n",
    "\n",
    "# quick validation\n",
    "ok_20 = submission.groupby(\"user_id\").size().eq(20).all()\n",
    "unique_ed = submission.groupby(\"user_id\")[\"edition_id\"].nunique().eq(20).all()\n",
    "unique_rank = submission.groupby(\"user_id\")[\"rank\"].nunique().eq(20).all()\n",
    "print(\"20 rows per user\", ok_20)\n",
    "print(\"unique edition_id\", unique_ed)\n",
    "print(\"unique rank\", unique_rank)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}